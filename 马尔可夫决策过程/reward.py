import numpy as np

np.random.seed(0)

# ==========================================================
# 1. 6状态 MRP (用于第一个价值计算)
# ==========================================================

# 定义状态转移概率矩阵 P (MRP)
# 这是一个 6x6 的 NumPy 矩阵
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # s6 是终止状态，自我转移概率为 1.0
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数 R (6个状态)
gamma = 0.5  # 定义折扣因子


# ==========================================================
# 2. 序列回报计算
# ==========================================================

# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报
def compute_return(start_index, chain, gamma):
    """
    计算给定序列的回报 G。
    chain: 状态序列 (索引从 1 开始)
    rewards: 奖励列表 (索引从 0 开始, 对应状态 1 到 6)
    gamma: 折扣因子
    """
    G = 0
    # 从序列末尾向前迭代,应用折扣因子
    for i in reversed(range(start_index, len(chain))):
        # chain[i] - 1 是状态的索引 (s1 -> 0, s6 -> 5)
        G = gamma * G + rewards[chain[i] - 1]
    return G


# 一个状态序列,s1-s2-s3-s6 (对应索引 1, 2, 3, 6)
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("根据本序列计算得到回报为：%s。" % G)

# ==========================================================
# 3. MDP 定义 (变量 P 已重命名为 P_mdp_dict)
# ==========================================================

S = ["s1", "s2", "s3", "s4", "s5"]  # 状态集合
A = ["保持s1", "前往s1", "前往s2", "前往s3", "前往s4", "前往s5", "概率前往"]  # 动作集合

# 状态转移函数 P_mdp_dict (字典形式, 用于 MDP 定义, 避免覆盖 MRP 矩阵 P)
P_mdp_dict = {
    "s1-保持s1-s1": 1.0,
    "s1-前往s2-s2": 1.0,
    "s2-前往s1-s1": 1.0,
    "s2-前往s3-s3": 1.0,
    "s3-前往s4-s4": 1.0,
    "s3-前往s5-s5": 1.0,
    "s4-前往s5-s5": 1.0,
    "s4-概率前往-s2": 0.2,
    "s4-概率前往-s3": 0.4,
    "s4-概率前往-s4": 0.4,
}
# 奖励函数 R
R = {
    "s1-保持s1": -1,
    "s1-前往s2": 0,
    "s2-前往s1": -1,
    "s2-前往s3": -2,
    "s3-前往s4": -2,
    "s3-前往s5": 0,
    "s4-前往s5": 10,
    "s4-概率前往": 1,
}
# MDP = (S, A, P_mdp_dict, R, gamma) # MDP定义元组可省略

# 策略1,随机策略
Pi_1 = {
    "s1-保持s1": 0.5,
    "s1-前往s2": 0.5,
    "s2-前往s1": 0.5,
    "s2-前往s3": 0.5,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.5,
    "s4-概率前往": 0.5,
}
# 策略2 (未使用)
Pi_2 = {
    "s1-保持s1": 0.6,
    "s1-前往s2": 0.4,
    "s2-前往s1": 0.3,
    "s2-前往s3": 0.7,
    "s3-前往s4": 0.5,
    "s3-前往s5": 0.5,
    "s4-前往s5": 0.1,
    "s4-概率前往": 0.9,
}


# 辅助函数 (未使用,但保留)
def join(str1, str2):
    return str1 + '-' + str2


# ==========================================================
# 4. 矩阵形式计算价值函数 (贝尔曼方程解析解)
# ==========================================================

# 转化后的 MRP 的状态转移矩阵 (由 MDP 在策略 Pi_1 下导出, 5x5 矩阵)
P_from_mdp_to_mrp = [
    [0.5, 0.5, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.5, 0.5],
    [0.0, 0.1, 0.2, 0.2, 0.5],
    [0.0, 0.0, 0.0, 0.0, 1.0],
]
P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)

# 转化后的 MRP 的奖励向量 (由 MDP 在策略 Pi_1 下导出, 5个状态)
R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]


def compute(P_matrix, rewards_vector, gamma, states_num):
    '''
    利用贝尔曼方程的矩阵形式计算解析解。
    P_matrix: MRP的状态转移矩阵 (必须是 numpy 数组)
    rewards_vector: 奖励向量 (列表或 numpy 数组)
    states_num: MRP的状态数
    '''
    # 确保 P_matrix 是 numpy 数组 (这是修复的关键)
    if not isinstance(P_matrix, np.ndarray):
        print("错误: 传入的 P 必须是 numpy 数组。当前 P 的类型为:", type(P_matrix))
        # 尝试转换,但这里我们假定外部调用已修复
        P_matrix = np.array(P_matrix)

    rewards_vector = np.array(rewards_vector).reshape((-1, 1))  # 将 rewards 写成列向量形式

    # 核心计算: V = (I - gamma * P)^-1 * R
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P_matrix),
                   rewards_vector)
    return value


# 第一次计算: 6状态 MRP (使用最开始定义的 P 矩阵)
V_mrp = compute(P, rewards, gamma, 6)
print("\n[结果 1] 6 状态 MRP 中每个状态价值分别为\n", V_mrp)

# 第二次计算: 5状态 MRP (由 MDP 在策略 Pi_1 下导出)
V_mdp = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)
print("\n[结果 2] MDP 中策略 Pi_1 下的每个状态价值分别为\n", V_mdp)
